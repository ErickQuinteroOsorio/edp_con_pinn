{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlza35ujwJa+skwK16iCib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErickQuinteroOsorio/edp_con_pinn/blob/main/notebooks/analisis_edp_con_pinn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Sd-XC-8yYZmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AS7Qjq1tYb-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 1: Initial settings for the heat equation\n",
        "#----------------------------------------------------\n",
        "import tensorflow as tf\n",
        "# We set seeds initially. This feature controls the randomization of\n",
        "# variables (e.g. initial weights of the network).\n",
        "# By doing it so, we can reproduce same results.\n",
        "tf.random.set_seed(123)\n",
        "# 100 equidistant points in the domain are created\n",
        "x = tf.linspace(0.0, 1.0, 100)\n",
        "# boundary conditions T(0)=T(1)=0 and \\kappa are introduced.\n",
        "bcs_x = [0.0, 1.0]\n",
        "bcs_T = [0.0, 0.0]\n",
        "\n",
        "bcs_x_tensor = tf.convert_to_tensor(bcs_x)[:, None]\n",
        "bcs_T_tensor = tf.convert_to_tensor(bcs_T)[:, None]\n",
        "kappa = 0.5\n",
        "\n",
        "# Number of iterations\n",
        "N = 1000\n",
        "# ADAM optimizer with learning rate of 0.005\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "# Function for creating the model\n",
        "def buildModel(num_hidden_layers, num_neurons_per_layer):\n",
        "  tf.keras.backend.set_floatx(\"float32\")\n",
        "\n",
        "  # Initialize a feedforward neural network\n",
        "  model = tf.keras.Sequential()\n",
        "  # Input is one dimensional (one spatial dimension)\n",
        "  # model.add(tf.keras.Input(1)) # así estaba en el libro y ya no funciona\n",
        "  model.add(tf.keras.Input(shape=(1,))) # Corrected line\n",
        "\n",
        "  # Append hidden layers\n",
        "  for _ in range(num_hidden_layers):\n",
        "    model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "                                    activation=tf.keras.activations.get(\"tanh\"),\n",
        "                                    kernel_initializer=\"glorot_normal\",\n",
        "                                    )\n",
        "    )\n",
        "\n",
        "  # Output is one-dimensional\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  return model\n",
        "\n",
        "\n",
        "# determine the model size (3 hidden layers with 32 neurons each)\n",
        "model = buildModel(3, 32)"
      ],
      "metadata": {
        "id": "us5lSQvDeQBz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 2: Loss function definition for the heat equation\n",
        "#------------------------------------------------------------\n",
        "# Boundary loss function\n",
        "def boundary_loss(bcs_x_tensor, bcs_T_tensor):\n",
        "  predicted_bcs = model(bcs_x_tensor)\n",
        "  mse_bcs = tf.reduce_mean(tf.square(predicted_bcs - bcs_T_tensor))\n",
        "  return mse_bcs\n",
        "\n",
        "# the first derivative of the prediction\n",
        "def get_first_deriv(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    T = model(x)\n",
        "  T_x = tape.gradient(T, x)\n",
        "  return T_x\n",
        "\n",
        "# the second derivative of the prediction\n",
        "def second_deriv(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(x)\n",
        "    T_x = get_first_deriv(x)\n",
        "  T_xx = tape.gradient(T_x, x)\n",
        "  return T_xx\n",
        "\n",
        "# Source term divided by \\kappa\n",
        "source_func = lambda x: (15 * x - 2) / kappa\n",
        "\n",
        "# Function for physics loss\n",
        "def physics_loss(x):\n",
        "  predicted_Txx = second_deriv(x)\n",
        "  mse_phys = tf.reduce_mean(tf.square(predicted_Txx + source_func(x)))\n",
        "  return mse_phys\n",
        "\n",
        "# Overall loss function\n",
        "def loss_func(x, bcs_x_tensor, bcs_T_tensor):\n",
        "  bcs_loss = boundary_loss(bcs_x_tensor, bcs_T_tensor)\n",
        "  phys_loss = physics_loss(x)\n",
        "  loss = bcs_loss + phys_loss\n",
        "  return loss"
      ],
      "metadata": {
        "id": "YZDYkdommJne"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Listing 3: Training of the heat equation model\n",
        "#-------------------------------------------------\n",
        "# taking gradients of the loss function\n",
        "def get_grad():\n",
        "  with tf.GradientTape() as tape:\n",
        "    # This tape is for derivatives with\n",
        "    # respect to trainable variables\n",
        "    # tape.watch(model.trainable_variables) # Removed redundant watch\n",
        "    Loss = loss_func(x, bcs_x_tensor, bcs_T_tensor)\n",
        "\n",
        "  g = tape.gradient(Loss, model.trainable_variables)\n",
        "  return Loss, g\n",
        "\n",
        "# optimizing and updating the weights of the model by using gradients\n",
        "def train_step():\n",
        "  # Compute current loss and gradient w.r.t. parameters\n",
        "  loss, grad_theta = get_grad()\n",
        "  # Perform gradient descent step\n",
        "  optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "# Training loop\n",
        "for i in range(N + 1):\n",
        "  loss = train_step()\n",
        "  # printing loss amount in each 100 epoch\n",
        "  if i % 100 == 0:\n",
        "    print(\"Epoch {:05d}: loss = {:10.8e}\".format(i, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVjI2ghXr_jr",
        "outputId": "a35f3549-e559-485c-a73a-00955eb6157e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 00000: loss = 1.99108078e+02\n",
            "Epoch 00100: loss = 1.53024683e+01\n",
            "Epoch 00200: loss = 3.80971245e-02\n",
            "Epoch 00300: loss = 3.48418951e-03\n",
            "Epoch 00400: loss = 1.71695207e-03\n",
            "Epoch 00500: loss = 1.28656975e-03\n",
            "Epoch 00600: loss = 1.09738461e-03\n",
            "Epoch 00700: loss = 2.69013480e-03\n",
            "Epoch 00800: loss = 9.23037704e-04\n",
            "Epoch 00900: loss = 1.22378662e-03\n",
            "Epoch 01000: loss = 1.18407246e-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aora con la red ya entrenada\n",
        "\n",
        "Once the training process is completed with the desired loss value, we can validate\n",
        "the output by performing one forward pass with a test dataset which is typically\n",
        "formed in the same domain as the training dataset. In our example, the training data\n",
        "was 100 equidistant points between 0 and 1.We can determine our test dataset as 200\n",
        "equidistant points in the same domain. Figure 5.5 depicts that the model’s prediction\n",
        "captures the analytical result."
      ],
      "metadata": {
        "id": "AfWLIEb41Jpp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Qyr6xS11XWm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}